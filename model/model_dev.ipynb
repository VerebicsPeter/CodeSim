{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VerebicsPeter/CodeSim/blob/main/model/model_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9rC5t03TgEv"
      },
      "source": [
        "# Code Similarity with Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHWMVk9vTgE3"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fyzd4Diwte7N",
        "outputId": "b8e55351-d5a0-48b5-e48c-bca7bc1fba3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-minifier\n",
            "  Downloading python_minifier-2.9.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
            "Downloading python_minifier-2.9.0-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-minifier\n",
            "Successfully installed python-minifier-2.9.0\n"
          ]
        }
      ],
      "source": [
        "# for data augmentation\n",
        "!pip install python-minifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NzFAEEP-mfv9",
        "outputId": "213cd6c7-eeb0-470b-ed12-eeaee5c2d505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch-metric-learning\n",
            "  Downloading pytorch_metric_learning-2.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.3.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (2.4.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2024.6.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
            "Downloading pytorch_metric_learning-2.6.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-2.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-metric-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNViP9VnTgE4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from pytorch_metric_learning import losses\n",
        "# Hugging Face Transformers (CodeBERT etc.)\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "# Libraries for augmenting data\n",
        "import python_minifier\n",
        "# Libraries for logging\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Iterable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtfdWw8jTgE7",
        "outputId": "49867d9b-c8e9-4f10-9636-6ebabcaf9d23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:', device)\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsY6hOil070N"
      },
      "source": [
        "## Dataset Access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS9pegh1dDj4"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "labeled_dataset_url   = f\"https://drive.google.com/uc?export=download&id={userdata.get('labeledDataset')}\"\n",
        "unlabeled_dataset_url = f\"https://drive.google.com/uc?export=download&id={userdata.get('unlabeledDataset')}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uws_FYLpwDr0"
      },
      "source": [
        "## Dataset and Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boNhtBMiTgFB"
      },
      "outputs": [],
      "source": [
        "# Code datasets (for labeled and unlabeled code snippets)\n",
        "\n",
        "\n",
        "def numeric_labels(labels) -> torch.Tensor:\n",
        "    \"\"\"Transform string labels to int labels for the NTXent loss function.\"\"\"\n",
        "    pos_labels = [ label for label in labels if label.endswith('1') ]\n",
        "    labels_map = { label: i for i,label in enumerate(sorted(set(pos_labels))) }\n",
        "    int_labels = [ labels_map.get(label, -1) for label in labels ]\n",
        "    int_labels = torch.Tensor(int_labels)\n",
        "    neg_indices = (int_labels == -1).nonzero(as_tuple=True)[0]\n",
        "    M = max(int_labels)\n",
        "    int_labels[neg_indices] = torch.arange(M + 1, M + 1 + len(neg_indices))\n",
        "    return int_labels\n",
        "\n",
        "# TODO: tokenize snippets\n",
        "class LabeledCodeDataset(Dataset):\n",
        "    def __init__(self, tokenizer, code_snippets, labels):\n",
        "        assert len(inputs) == len(labels)\n",
        "        MAX_LEN = tokenizer.model_max_length\n",
        "        inputs = tokenizer(\n",
        "            code_snippets,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_LEN + 1,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        # Find valid pairs that don't exceed model max length\n",
        "        input_mask = inputs[\"attention_mask\"].sum(dim=1) <= MAX_LEN\n",
        "        self.inputs = {k: v[input_mask, :MAX_LEN] for k, v in inputs.items()}\n",
        "        # Move tensors to the specified device\n",
        "        self.inputs = {k: v.to(device) for k, v in self.inputs.items()}\n",
        "        self.labels = num_labels(labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input = self.inputs[idx]\n",
        "        label = self.labels[idx]\n",
        "        return input, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.codes)\n",
        "\n",
        "    @classmethod\n",
        "    def from_csv_data(cls, path: str, tokenizer, sample_size=0):\n",
        "        df = pd.read_csv(path)\n",
        "        print(df.shape)\n",
        "        if sample_size:\n",
        "            print('sampling dataframe...')\n",
        "            df = df.sample(sample_size, ignore_index=True)\n",
        "            print(df.shape)\n",
        "        code = df['source']\n",
        "        lbls = df['label']\n",
        "        return cls(tokenizer, code, lbls)\n",
        "\n",
        "\n",
        "class UnlabeledCodeDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: (\n",
        "            transformers.PreTrainedTokenizer |\n",
        "            transformers.PreTrainedTokenizerFast\n",
        "        ),\n",
        "        ref_codes: Iterable[str],\n",
        "        aug_codes: Iterable[str],\n",
        "    ):\n",
        "        assert len(ref_codes) == len(aug_codes)\n",
        "\n",
        "        MAX_LEN = tokenizer.model_max_length\n",
        "\n",
        "        # Tokenize all codes at once and filter based on max length\n",
        "        ref_encodings = tokenizer(\n",
        "            ref_codes,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_LEN + 1,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        aug_encodings = tokenizer(\n",
        "            aug_codes,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_LEN + 1,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Find valid pairs that don't exceed model max length\n",
        "        ref_mask = ref_encodings[\"attention_mask\"].sum(dim=1) <= MAX_LEN\n",
        "        aug_mask = aug_encodings[\"attention_mask\"].sum(dim=1) <= MAX_LEN\n",
        "        valid_mask = ref_mask & aug_mask\n",
        "\n",
        "        # Keep only valid encodings\n",
        "        self.ref_inputs = {k: v[valid_mask] for k, v in ref_encodings.items()}\n",
        "        self.aug_inputs = {k: v[valid_mask] for k, v in aug_encodings.items()}\n",
        "\n",
        "        # Remove padding tokens from the last valid position\n",
        "        self.ref_inputs[\"input_ids\"] = self.ref_inputs[\"input_ids\"][:, :MAX_LEN]\n",
        "        self.aug_inputs[\"input_ids\"] = self.aug_inputs[\"input_ids\"][:, :MAX_LEN]\n",
        "\n",
        "        # Also adjust the attention masks\n",
        "        self.ref_inputs[\"attention_mask\"] = self.ref_inputs[\"attention_mask\"][\n",
        "            :, :MAX_LEN\n",
        "        ]\n",
        "        self.aug_inputs[\"attention_mask\"] = self.aug_inputs[\"attention_mask\"][\n",
        "            :, :MAX_LEN\n",
        "        ]\n",
        "\n",
        "        # Move tensors to the specified device\n",
        "        self.ref_inputs = {k: v.to(device) for k, v in self.ref_inputs.items()}\n",
        "        self.aug_inputs = {k: v.to(device) for k, v in self.aug_inputs.items()}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return both reference and augmented code inputs for a given index\n",
        "        ref_input = {k: v[idx] for k, v in self.ref_inputs.items()}\n",
        "        aug_input = {k: v[idx] for k, v in self.aug_inputs.items()}\n",
        "        return ref_input, aug_input\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ref_inputs[\"input_ids\"].shape[0]\n",
        "\n",
        "    @classmethod\n",
        "    def from_csv_data(cls, path: str, tokenizer, aug_func,\n",
        "                      sample_size=0):\n",
        "        df = pd.read_csv(path)\n",
        "        print(df.shape)\n",
        "        if sample_size:\n",
        "            print('sampling dataframe...')\n",
        "            df = df.sample(sample_size, ignore_index=True)\n",
        "            print(df.shape)\n",
        "        ref_codes = df['source']\n",
        "        aug_codes = df['source'].apply(aug_func)\n",
        "        return cls(tokenizer, ref_codes.to_list(), aug_codes.to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V9lNe8lEcV4"
      },
      "outputs": [],
      "source": [
        "def minify(code: str) -> str:\n",
        "    try: return python_minifier.minify(code)\n",
        "    except Exception as error:\n",
        "        #print(f'Error while minifying: {error}')  # use a log file for this\n",
        "        pass\n",
        "    return code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0yWCEhrTgE-"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I66sH2KATgE_"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "\n",
        "IS_SELF_SUPERVISED = True\n",
        "\n",
        "\n",
        "def embedding_pipeline(transformer):\n",
        "    \"\"\"Create an embedding function with a `tokenizer` and `transformer`.\"\"\"\n",
        "    def pipeline(inputs: dict):\n",
        "        with torch.device(device):\n",
        "            return transformer(**inputs)\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "class CodeSimilarityModel(nn.Module):\n",
        "    def __init__(self,\n",
        "        embedding_pipeline,\n",
        "        in_feat=768,  # depends on the embedding pipeline\n",
        "        fc_hidden_size=512,\n",
        "        mlp_sizes=(256, 128, 64),\n",
        "        out_feat=16,\n",
        "        dropout_rate=0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_pipeline = embedding_pipeline\n",
        "        self.in_feat = in_feat\n",
        "        # Non linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(dropout_rate)\n",
        "        # MLP 'projection head'\n",
        "        mlp_layers = []\n",
        "        mlp_layers.append(nn.Linear(mlp_sizes[0], mlp_sizes[1]))\n",
        "        mlp_layers.extend([self.relu, self.drop])\n",
        "        mlp_layers.append(nn.Linear(mlp_sizes[1], mlp_sizes[2]))\n",
        "        mlp_layers.extend([self.relu, self.drop])\n",
        "        mlp_layers.append(nn.Linear(mlp_sizes[2], out_feat))\n",
        "        self.mlp = nn.Sequential(*mlp_layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.device(device):\n",
        "            # pass through linear layers\n",
        "            x = self.mlp(x)\n",
        "            return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2fugQParBXm"
      },
      "source": [
        "## NTXent Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CxjYpjrAEw"
      },
      "outputs": [],
      "source": [
        "ntxent_loss = losses.NTXentLoss(temperature=0.5)\n",
        "# Wrap the loss function if needed\n",
        "ntxent_loss = losses.SelfSupervisedLoss(ntxent_loss) if IS_SELF_SUPERVISED else ntxent_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCRPRlQav58Y"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOWTBhaXqOMt",
        "outputId": "a65aad13-f283-4df1-8109-d96f87481faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(34806, 3)\n",
            "sampling dataframe...\n",
            "(25000, 3)\n",
            "augmenting dataframe...\n",
            "(40048, 3)\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset\n",
        "SAMPLE_SIZE = 25_000\n",
        "\n",
        "if IS_SELF_SUPERVISED:\n",
        "    dataset = LabeledCodeDataset.from_csv_data(path=labeled_dataset_url, sample_size=SAMPLE_SIZE)\n",
        "else:\n",
        "    dataset = UnlabeledCodeDataset.from_csv_data(path=unlabeled_dataset_url, sample_size=SAMPLE_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVma52Qn-5Zl"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "tsize = int(0.8 * len(dataset))\n",
        "vsize = len(dataset) - tsize\n",
        "training_data, validation_data = random_split(dataset, [tsize, vsize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MuquE1Hf-01p"
      },
      "outputs": [],
      "source": [
        "# Create the data loaders\n",
        "BATCH_SIZE = 20  # NOTE: Bigger batch size generally leads to better results in contrastive learning\n",
        "SHUFFLE = True\n",
        "training_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
        "validation_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enULAgCJTgFC"
      },
      "outputs": [],
      "source": [
        "# Training loop for the NTXEnt loss function\n",
        "\n",
        "def compute_loss(batched_data, model, loss_func):\n",
        "    \"\"\"Computes the loss value for a batch of data.\"\"\"\n",
        "    if isinstance(loss_func, losses.SelfSupervisedLoss):\n",
        "        ref_input, aug_input = batched_data\n",
        "        ref_emb = model(ref_input)  # fc1-fc2, MLP\n",
        "        aug_emb = model(aug_input)  # fc1-fc2, MLP\n",
        "        loss = loss_func(ref_emb, aug_emb)\n",
        "        return loss\n",
        "    else:\n",
        "        inputs, labels = batched_data\n",
        "        embeddings = model(inputs)  # fc1-fc2, MLP\n",
        "        loss = loss_func(embeddings, labels)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    model: CodeSimilarityModel,\n",
        "    loader: DataLoader,\n",
        "    loss_func,\n",
        "    optimizer,\n",
        "    epochs: int                  = 0,     # number of epochs so far (for logging),\n",
        "    writer: SummaryWriter | None = None,  # for logging loss values,\n",
        "):\n",
        "    \"\"\"Trains the model for one epoch.\"\"\"\n",
        "    def get_last_loss(n_batches, c_batches, batch, acc_loss):\n",
        "        if batch % c_batches == c_batches - 1:\n",
        "            return 0, acc_loss / c_batches\n",
        "        elif batch == N_BATCHES - 1:\n",
        "            return 0, acc_loss / (n_batches % c_batches)\n",
        "        return acc_loss, 0\n",
        "\n",
        "    def write_loss(writer, epoch, n_batches, batch, last_loss):\n",
        "        # Log the average loss over the last  batches\n",
        "        print('',f'Batch: {batch + 1}/{n_batches}, Loss: {last_loss}')  # use a log file for this\n",
        "        if writer is not None:\n",
        "            writer.add_scalar(\"loss/train\", last_loss, epochs * n_batches + batch + 1)\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "    N_BATCHES = len(loader)  # Number of batches\n",
        "    C_BATCHES = 50  # Number of batches over which the logged loss is cumulated\n",
        "    sum_loss = 0  # Loss accumulated per EPOCH\n",
        "    acc_loss = 0  # Loss accumulated per last 25 batches\n",
        "    progress_bar = tqdm(range(N_BATCHES))\n",
        "    for i, data in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        loss = compute_loss(data, model, loss_func, is_labeled_data=IS_SUPERVISED)\n",
        "        # Adjust the weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Increase loss accumulator\n",
        "        loss_val = loss.item()\n",
        "        acc_loss += loss_val; sum_loss += loss_val\n",
        "        # Update the loss accumulator and log the last loss\n",
        "        progress_bar.update(1)\n",
        "        acc_loss, last_loss = get_last_loss(N_BATCHES, C_BATCHES, i, acc_loss)\n",
        "        if last_loss: write_loss(writer, epochs, N_BATCHES, i, last_loss)\n",
        "    # Return the average loss in the epoch\n",
        "    avg_loss = sum_loss / N_BATCHES\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def validate(\n",
        "    model: CodeSimilarityModel,\n",
        "    loader: DataLoader,\n",
        "    loss_func,\n",
        "):\n",
        "    \"\"\"Validates the model for one epoch.\"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        sum_loss = 0\n",
        "        for data in loader:\n",
        "            loss = compute_loss(data, model, loss_func, is_labeled_data=IS_SUPERVISED)\n",
        "            sum_loss += loss.item()\n",
        "        avg_loss = sum_loss / len(loader)\n",
        "        return avg_loss\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: CodeSimilarityModel,\n",
        "    dataloaders,\n",
        "    loss_func,\n",
        "    optimizer, scheduler,\n",
        "    epochs: int = 5,\n",
        "):\n",
        "    writer = None #SummaryWriter()\n",
        "    tLosses, vLosses = [], []\n",
        "    training_loader, validation_loader = dataloaders\n",
        "    for epoch in range(epochs):\n",
        "        print(f'EPOCH {epoch + 1}/{epochs}')\n",
        "        # Train then validate\n",
        "        avg_tLoss = train_epoch(model, training_loader, loss_func, optimizer, epoch, writer)\n",
        "        avg_vLoss = validate(model, validation_loader, loss_func)\n",
        "        # Adjust the LR scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        # Log the losses\n",
        "        print(f\"EPOCH {epoch + 1}/{epochs}, AVG loss: {avg_tLoss}, AVG validation loss: {avg_vLoss}\")\n",
        "        tLosses.append(avg_tLoss)\n",
        "        vLosses.append(avg_vLoss)\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "    return tLosses, vLosses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmsA8R0JTgFD"
      },
      "outputs": [],
      "source": [
        "# Create embedding pipeline, model, tokenizer, and optimizer\n",
        "\n",
        "\"\"\"\n",
        "Other checkpoints:\n",
        "- \"microsoft/codebert-base\"\n",
        "- \"huggingface/CodeBERTa-small-v1\"\n",
        "\"\"\"\n",
        "\n",
        "pretrained_model=\"neulab/codebert-python\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "transformer = AutoModel.from_pretrained(pretrained_model).to(device)\n",
        "transformer.eval()\n",
        "embedding_pipeline = embedding_pipeline(tokenizer, transformer)\n",
        "model = CodeSimilarityModel(embedding_pipeline).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhBywFg2_42t"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "epochs = 4\n",
        "loss_func = ntxent_loss\n",
        "losses = train(model, (training_loader, validation_loader), loss_func, optimizer, lr_scheduler, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTKcoNDWTgFE"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses[0])\n",
        "plt.plot(losses[1])\n",
        "plt.legend(['training loss', 'validation loss'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7srChA6YkJJK"
      },
      "source": [
        "## Notes\n",
        "\n",
        "### Training results with different parameters\n",
        "\n",
        "| Checkpoint |`T`| Model Structure | Epochs | Training Loss | Validation Loss |\n",
        "| ---------- | - | --------------- | ------ | ------------- | --------------- |\n",
        "| CodeBERTa Small v1 | 0.07 | TFM → mean pool → MLP w/ batchnorm | - | ~1.6 | - |\n",
        "| CodeBERTa Small v1 | 0.07 | TFM → lin1 → lin2 → MLP w/ batchnorm → max pool | - | ~1.4 | - |\n",
        "| CodeBERTa Small v1 | 0.50 | (frozen) TFM's pooler output → layernorm → lin1 → lin2 → MLP  | 6 |  ~1.5 | ~1.35 |\n",
        "\n",
        "`T` is the temperature hyperparameter of the NTXent loss function.\n",
        "\n",
        "### Data TODOs\n",
        "- ❎ - Throw away code snippets that are too long\n",
        "- ❎ - A lot of codes snippets mined from github can't be minified, filter unlabeled code dataset!\n",
        "- ❎ - Pre calculate data augmentations\n",
        "\n",
        "### Model TODOs\n",
        "- ❎ - Try training with transformer's `pooler_output`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}