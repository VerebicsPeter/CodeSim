{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VerebicsPeter/CodeSim/blob/main/model/model_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9rC5t03TgEv"
      },
      "source": [
        "# Code Similarity with Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHWMVk9vTgE3"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fyzd4Diwte7N",
        "outputId": "b8e55351-d5a0-48b5-e48c-bca7bc1fba3c"
      },
      "outputs": [],
      "source": [
        "#!pip install python-minifier  # for data augmentation\n",
        "#!pip install pytorch-metric-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNViP9VnTgE4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#import numpy as np\n",
        "import pandas as pd\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from pytorch_metric_learning import losses\n",
        "# Hugging Face Transformers (CodeBERT etc.)\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "# Libraries for augmenting data\n",
        "import python_minifier\n",
        "# Libraries for logging\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Iterable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtfdWw8jTgE7",
        "outputId": "49867d9b-c8e9-4f10-9636-6ebabcaf9d23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:', device)\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsY6hOil070N"
      },
      "source": [
        "## Dataset Access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS9pegh1dDj4"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "labeled_dataset_url = f\"https://drive.google.com/uc?export=download&id={userdata.get('labeledDataset')}\"\n",
        "unlabeled_dataset_url = f\"https://drive.google.com/uc?export=download&id={userdata.get('unlabeledDataset')}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uws_FYLpwDr0"
      },
      "source": [
        "## Dataset and Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boNhtBMiTgFB"
      },
      "outputs": [],
      "source": [
        "# Code datasets (for labeled and unlabeled code snippets)\n",
        "\n",
        "\n",
        "def numeric_labels(labels: Iterable[str]) -> torch.Tensor:\n",
        "    \"\"\"Transform string labels to int labels for the NTXent loss function.\"\"\"\n",
        "    pos_labels = [ label for label in labels if label.endswith('1') ]\n",
        "    labels_map = { label: i for i,label in enumerate(sorted(set(pos_labels))) }\n",
        "    int_labels = [ labels_map.get(label, -1) for label in labels ]\n",
        "    int_labels = torch.Tensor(int_labels)\n",
        "    neg_indices = (int_labels == -1).nonzero(as_tuple=True)[0]\n",
        "    M = max(int_labels)\n",
        "    int_labels[neg_indices] = torch.arange(M + 1, M + 1 + len(neg_indices))\n",
        "    return int_labels\n",
        "\n",
        "\n",
        "def get_batch_encodings(\n",
        "    codes: Iterable[str], \n",
        "    tokenizer: transformers.PreTrainedTokenizer | transformers.PreTrainedTokenizerFast,\n",
        "    device: str = \"cpu\"\n",
        ") -> transformers.BatchEncoding:\n",
        "    MAX_LEN = tokenizer.model_max_length\n",
        "    inputs = tokenizer(\n",
        "        codes,\n",
        "        truncation=True,\n",
        "        # Pad to \"MAX_LEN + 1\" to detect sequences that are too long\n",
        "        padding=\"max_length\", max_length=MAX_LEN + 1,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    \n",
        "    # Mask sequences that are longer than \"MAX_LEN\"\n",
        "    l_mask = inputs[\"attention_mask\"].sum(dim=1) <= MAX_LEN\n",
        "    \n",
        "    inputs = { k: v[l_mask, :MAX_LEN] for k, v in inputs.items() }\n",
        "    # Move tensors to the specified device\n",
        "    inputs = { k: v.to(device) for k, v in inputs.items() }\n",
        "    \n",
        "    return inputs\n",
        "\n",
        "\n",
        "class LabeledCodeDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: transformers.PreTrainedTokenizer | transformers.PreTrainedTokenizerFast, \n",
        "        codes : Iterable[str],\n",
        "        labels: Iterable[str]\n",
        "    ):\n",
        "        assert len(codes) == len(labels)\n",
        "        self.inputs = get_batch_encodings(codes, tokenizer, device)\n",
        "        self.labels = numeric_labels(labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input = {k: v[idx] for k, v in self.inputs.items()}\n",
        "        label = self.labels[idx]\n",
        "        return input, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs['input_ids'].shape[0])\n",
        "\n",
        "    @classmethod\n",
        "    def from_csv_data(cls, path: str, tokenizer, sample_size=0):\n",
        "        df = pd.read_csv(path)\n",
        "        print(df.shape)\n",
        "        \n",
        "        if sample_size:\n",
        "            print('sampling dataframe...')\n",
        "            df = df.sample(sample_size, ignore_index=True)\n",
        "            print(df.shape)\n",
        "        \n",
        "        codes = df['source']\n",
        "        codes = codes.to_list()\n",
        "        \n",
        "        labels = df['label']\n",
        "        labels = labels.to_list()\n",
        "        \n",
        "        return cls(tokenizer, codes, labels)\n",
        "\n",
        "\n",
        "class UnlabeledCodeDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: transformers.PreTrainedTokenizer | transformers.PreTrainedTokenizerFast,\n",
        "        ref_codes: Iterable[str],\n",
        "        aug_codes: Iterable[str],\n",
        "    ):\n",
        "        assert len(ref_codes) == len(aug_codes)\n",
        "        self.ref_inputs = get_batch_encodings(ref_codes, tokenizer, device)\n",
        "        self.aug_inputs = get_batch_encodings(aug_codes, tokenizer, device)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # Return both reference and augmented code inputs for a given index\n",
        "        ref_input = {k: v[idx] for k, v in self.ref_inputs.items()}\n",
        "        aug_input = {k: v[idx] for k, v in self.aug_inputs.items()}\n",
        "        return ref_input, aug_input\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ref_inputs[\"input_ids\"].shape[0]\n",
        "\n",
        "    @classmethod\n",
        "    def from_csv_data(cls, path: str, tokenizer, aug_func, sample_size=0):\n",
        "        df = pd.read_csv(path)\n",
        "        print(df.shape)\n",
        "        \n",
        "        if sample_size:\n",
        "            print('sampling dataframe...')\n",
        "            df = df.sample(sample_size, ignore_index=True)\n",
        "            print(df.shape)\n",
        "        \n",
        "        ref_codes = df['source']\n",
        "        aug_codes = df['source'].apply(aug_func)\n",
        "        \n",
        "        return cls(tokenizer, ref_codes.to_list(), aug_codes.to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V9lNe8lEcV4"
      },
      "outputs": [],
      "source": [
        "def minify(code: str) -> str:\n",
        "    try: return python_minifier.minify(code)\n",
        "    except Exception as error:\n",
        "        #print(f'Error while minifying: {error}')  # TODO: use a log file for this\n",
        "        pass\n",
        "    return code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0yWCEhrTgE-"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I66sH2KATgE_"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "\n",
        "IS_SELF_SUPERVISED = True\n",
        "\n",
        "\n",
        "def embedding_pipeline(transformer):\n",
        "    \"\"\"Create an embedding function with a `tokenizer` and `transformer`.\"\"\"\n",
        "    def pipeline(inputs: transformers.BatchEncoding):\n",
        "        with torch.device(device):\n",
        "            return transformer(**inputs)\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "class CodeSimilarityModel(nn.Module):\n",
        "    def __init__(self,\n",
        "        embedding_pipeline,\n",
        "        in_feat=768,  # depends on the embedding pipeline\n",
        "        mlp_sizes=(256, 128),\n",
        "        out_feat=32,\n",
        "        dropout_rate=0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_pipeline = embedding_pipeline\n",
        "        self.in_feat = in_feat\n",
        "        # Non linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(dropout_rate)\n",
        "        # MLP 'projection head'\n",
        "        mlp_layers = []\n",
        "        mlp_layers.append(nn.Linear(in_feat, mlp_sizes[1]))\n",
        "        \n",
        "        mlp_layers.extend([self.relu, self.drop])\n",
        "        mlp_layers.append(nn.Linear(mlp_sizes[1], mlp_sizes[2]))\n",
        "        \n",
        "        mlp_layers.extend([self.relu, self.drop])\n",
        "        mlp_layers.append(nn.Linear(mlp_sizes[2], out_feat))\n",
        "        \n",
        "        self.mlp = nn.Sequential(*mlp_layers)\n",
        "    \n",
        "    \n",
        "    def embed(self, inputs: transformers.BatchEncoding) -> torch.Tensor:\n",
        "        output = self.embedding_pipeline(inputs)\n",
        "        return output.pooler_output\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.device(device):\n",
        "            # pass through linear layers\n",
        "            x = self.mlp(x)\n",
        "            return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2fugQParBXm"
      },
      "source": [
        "## NTXent Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CxjYpjrAEw"
      },
      "outputs": [],
      "source": [
        "ntxent_loss = losses.NTXentLoss(temperature=0.5)\n",
        "# Wrap the NTXent loss function if needed\n",
        "ntxent_loss = losses.SelfSupervisedLoss(ntxent_loss) if IS_SELF_SUPERVISED else ntxent_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCRPRlQav58Y"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enULAgCJTgFC"
      },
      "outputs": [],
      "source": [
        "# Training loop for the NTXEnt loss function\n",
        "\n",
        "def compute_loss(batched_data, model, loss_func):\n",
        "    \"\"\"Computes the loss value for a batch of data.\"\"\"\n",
        "    if isinstance(loss_func, losses.SelfSupervisedLoss):\n",
        "        ref_input, aug_input = batched_data\n",
        "        ref_emb = model.embed(ref_input)  # transformer\n",
        "        aug_emb = model.embed(aug_input)  # transformer\n",
        "        ref_emb = model(ref_emb)  # MLP\n",
        "        aug_emb = model(aug_emb)  # MLP\n",
        "        loss = loss_func(ref_emb, aug_emb)\n",
        "        return loss\n",
        "    else:\n",
        "        inputs, labels = batched_data\n",
        "        embeddings = model.embed(inputs)  # transformer\n",
        "        embeddings = model(embeddings)  # MLP\n",
        "        loss = loss_func(embeddings, labels)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    model: CodeSimilarityModel,\n",
        "    loader: DataLoader,\n",
        "    loss_func,\n",
        "    optimizer,\n",
        "    epochs: int = 0,  # number of epochs so far (for logging)\n",
        "    writer: SummaryWriter | None = None,  # for logging loss values\n",
        "):\n",
        "    \"\"\"Trains the model for one epoch.\"\"\"\n",
        "    \n",
        "    def get_last_loss(n_batches, c_batches, batch, acc_loss):\n",
        "        if batch % c_batches == c_batches - 1:\n",
        "            return 0, acc_loss / c_batches\n",
        "        elif batch == N_BATCHES - 1:\n",
        "            return 0, acc_loss / (n_batches % c_batches)\n",
        "        return acc_loss, 0\n",
        "\n",
        "    def write_loss(writer, n_batches, batch, last_loss):\n",
        "        # Log the average loss over the last  batches\n",
        "        # TODO: use a log file for this\n",
        "        print('',f'Batch: {batch + 1}/{n_batches}, Loss: {last_loss}')\n",
        "        if writer is not None:\n",
        "            writer.add_scalar(\"loss/train\", last_loss, epochs * n_batches + batch + 1)\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "    N_BATCHES = len(loader)  # Number of batches\n",
        "    C_BATCHES = 50  # Number of batches over which the logged loss is cumulated\n",
        "    sum_loss = 0  # Loss accumulated per EPOCH\n",
        "    acc_loss = 0  # Loss accumulated per last 25 batches\n",
        "    progress_bar = tqdm(range(N_BATCHES))\n",
        "    for i, data in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        loss = compute_loss(data, model, loss_func)\n",
        "        # Adjust the weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Increase loss accumulator\n",
        "        loss_val = loss.item()\n",
        "        acc_loss += loss_val; sum_loss += loss_val\n",
        "        # Update the loss accumulator and log the last loss\n",
        "        progress_bar.update(1)\n",
        "        acc_loss, last_loss = get_last_loss(N_BATCHES, C_BATCHES, i, acc_loss)\n",
        "        if last_loss: write_loss(writer, epochs, N_BATCHES, i, last_loss)\n",
        "    # Return the average loss in the epoch\n",
        "    avg_loss = sum_loss / N_BATCHES\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def validate(\n",
        "    model: CodeSimilarityModel,\n",
        "    loader: DataLoader,\n",
        "    loss_func,\n",
        "):\n",
        "    \"\"\"Validates the model for one epoch.\"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        sum_loss = 0\n",
        "        for data in loader:\n",
        "            loss = compute_loss(data, model, loss_func)\n",
        "            sum_loss += loss.item()\n",
        "        avg_loss = sum_loss / len(loader)\n",
        "        return avg_loss\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: CodeSimilarityModel,\n",
        "    dataloaders,\n",
        "    loss_func,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    epochs: int = 5,\n",
        "):\n",
        "    writer = None #SummaryWriter()\n",
        "    tLosses, vLosses = [], []\n",
        "    training_loader, validation_loader = dataloaders\n",
        "    for epoch in range(epochs):\n",
        "        print(f'EPOCH {epoch + 1}/{epochs}')\n",
        "        # Train then validate\n",
        "        avg_tLoss = train_epoch(model, training_loader, loss_func, optimizer, epoch, writer)\n",
        "        avg_vLoss = validate(model, validation_loader, loss_func)\n",
        "        # Adjust the LR scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        # Log the losses\n",
        "        print(f\"EPOCH {epoch + 1}/{epochs}, AVG loss: {avg_tLoss}, AVG validation loss: {avg_vLoss}\")\n",
        "        tLosses.append(avg_tLoss)\n",
        "        vLosses.append(avg_vLoss)\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "    return tLosses, vLosses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PRETRAINED_MODEL=\"neulab/codebert-python\"\n",
        "\"\"\"\n",
        "Other models:\n",
        " \"microsoft/codebert-base\"\n",
        " \"huggingface/CodeBERTa-small-v1\"\n",
        "\"\"\"\n",
        "\n",
        "# Tokenizer is created first for preprocessing\n",
        "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOWTBhaXqOMt",
        "outputId": "a65aad13-f283-4df1-8109-d96f87481faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(34806, 3)\n",
            "sampling dataframe...\n",
            "(25000, 3)\n",
            "augmenting dataframe...\n",
            "(40048, 3)\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset\n",
        "\n",
        "SAMPLE_SIZE = 25_000\n",
        "\n",
        "if IS_SELF_SUPERVISED:\n",
        "    dataset = LabeledCodeDataset.from_csv_data(\n",
        "        path=labeled_dataset_url,\n",
        "        tokenizer=tokenizer,\n",
        "        sample_size=SAMPLE_SIZE\n",
        "    )\n",
        "else:\n",
        "    dataset = UnlabeledCodeDataset.from_csv_data(\n",
        "        path=unlabeled_dataset_url,\n",
        "        tokenizer=tokenizer,\n",
        "        sample_size=SAMPLE_SIZE,\n",
        "        aug_func=minify\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVma52Qn-5Zl"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "tsize = int(0.8 * len(dataset))\n",
        "vsize = len(dataset) - tsize\n",
        "training_data, validation_data = random_split(dataset, [tsize, vsize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MuquE1Hf-01p"
      },
      "outputs": [],
      "source": [
        "# Create the data loaders\n",
        "\n",
        "# NOTE: Bigger batch size generally leads to better results in contrastive learning\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE = True\n",
        "\n",
        "training_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
        "validation_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmsA8R0JTgFD"
      },
      "outputs": [],
      "source": [
        "# Create embedding pipeline, model, optimizer and scheduler\n",
        "\n",
        "transformer = AutoModel.from_pretrained(PRETRAINED_MODEL).to(device)\n",
        "transformer.eval()\n",
        "\n",
        "emb_pipeline = embedding_pipeline(transformer)\n",
        "\n",
        "model = CodeSimilarityModel(emb_pipeline).to(device)\n",
        "m_optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
        "m_scheduler = torch.optim.lr_scheduler.StepLR(m_optimizer, step_size=4, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhBywFg2_42t"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "losses = train(\n",
        "    model,\n",
        "    dataloaders=(training_loader, validation_loader),\n",
        "    loss_fun=ntxent_loss,\n",
        "    optimizer=m_optimizer,\n",
        "    scheduler=m_scheduler,\n",
        "    epochs=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTKcoNDWTgFE"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses[0])\n",
        "plt.plot(losses[1])\n",
        "plt.legend(['training loss', 'validation loss'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7srChA6YkJJK"
      },
      "source": [
        "## Notes\n",
        "\n",
        "### Training results with different parameters\n",
        "\n",
        "| Pretrained Model |`T`| Model Structure | Epochs | Training Loss | Validation Loss |\n",
        "| ---------- | - | --------------- | ------ | ------------- | --------------- |\n",
        "| CodeBERTa Small v1 | 0.07 | TFM → mean pool → MLP w/ batchnorm | - | ~1.6 | - |\n",
        "| CodeBERTa Small v1 | 0.07 | TFM → lin1 → lin2 → MLP w/ batchnorm → max pool | - | ~1.4 | - |\n",
        "| CodeBERTa Small v1 | 0.50 | (frozen) TFM's pooler output → layernorm → lin1 → lin2 → MLP  | 6 |  ~1.5 | ~1.35 |\n",
        "\n",
        "`T` is the temperature hyperparameter of the NTXent loss function.\n",
        "\n",
        "### Data TODOs\n",
        "- ❎ - A lot of codes snippets mined from github can't be minified, filter unlabeled code dataset!\n",
        "\n",
        "### Model TODOs\n",
        "- ❎ - Try training with transformer's `pooler_output`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
