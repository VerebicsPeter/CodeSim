{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Similarity with Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = \"microsoft/codebert-base\"\n",
    "tokenizer, model = AutoTokenizer.from_pretrained(PRETRAINED_MODEL), AutoModel.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_embedding(code: str):\n",
    "    inputs = tokenizer(code, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        outputs = outputs.last_hidden_state.mean(dim=1)  # [1, 9, 768] mean pooled -> [1, 768]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity:\n",
      "- positive pair: tensor([0.9913])\n",
      "- negative pair: tensor([0.8534])\n",
      "Pairwise distance:\n",
      "- positive pair: tensor([2.3960])\n",
      "- negative pair: tensor([10.0942])\n"
     ]
    }
   ],
   "source": [
    "# Example code snippets and pairs\n",
    "\n",
    "code_1 = \"def add(a, b): return a + b\"\n",
    "code_2 = \"def sum(x, y): return x + y\"\n",
    "code_3 = \"print('Hello, World!')\"\n",
    "\n",
    "ppair = code_1, code_2\n",
    "npair = code_1, code_3\n",
    "\n",
    "ppair_emb = tuple(map(get_code_embedding, ppair))\n",
    "npair_emb = tuple(map(get_code_embedding, npair))\n",
    "\n",
    "# Example similarity and distance calculation\n",
    "\n",
    "p_cosine_sim = F.cosine_similarity(*ppair_emb)\n",
    "n_cosine_sim = F.cosine_similarity(*npair_emb)\n",
    "p_pair_dist = F.pairwise_distance(*ppair_emb)\n",
    "n_pair_dist = F.pairwise_distance(*npair_emb)\n",
    "\n",
    "print('Cosine similarity:')\n",
    "print('- positive pair:', p_cosine_sim)\n",
    "print('- negative pair:', n_cosine_sim)\n",
    "\n",
    "print('Pairwise distance:')\n",
    "print('- positive pair:', p_pair_dist)\n",
    "print('- negative pair:', n_pair_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classic Contrastive Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(x1, x2, label: float, margin: float):\n",
    "    \"\"\"\n",
    "    The label indicates whether the pair is negative.\n",
    "        - 1.0 means positive\n",
    "        - 0.0 means negative\n",
    "    \n",
    "    The loss is is calculated like this:\n",
    "        `dist ** 2` if the label is positive `min(margin - dist)` otherwise\n",
    "    \"\"\"\n",
    "    dist = F.pairwise_distance(x1, x2)\n",
    "    loss = label * torch.pow(dist, 2) + (1 - label) * torch.pow(torch.clamp(margin - dist, min=0.0), 2)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# TODO\n",
    "\n",
    "class CodeSimilarityModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"microsoft/codebert-base\", hidden_size=768):\n",
    "        # NOTE: The base value for hidden_size is the size of size of the pooled output of the transformer\n",
    "        super(CodeSimilarityModel, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained tokenizer and transformer model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        # Additional layers for fine-tuning the embeddings\n",
    "        self.fc1     = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2     = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def _tokenize(self, code: str):\n",
    "        return self.tokenizer(code, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "    \n",
    "    \n",
    "    def forward(self, code1: str, code2: str):\n",
    "        # TODO: Only passing one tensor through the network MAY lead to better training time ...\n",
    "        \n",
    "        # Tokenize both code snippets\n",
    "        inputs1 = self._tokenize(code1)\n",
    "        inputs2 = self._tokenize(code2)\n",
    "        \n",
    "        # Get the transformer outputs for both inputs\n",
    "        transformer_outputs1 = self.transformer(**inputs1)\n",
    "        transformer_outputs2 = self.transformer(**inputs2)\n",
    "        \n",
    "        # Pool the transformer outputs (mean pooling)\n",
    "        pooled_output1 = transformer_outputs1.last_hidden_state.mean(dim=1)\n",
    "        pooled_output2 = transformer_outputs2.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        # Pass through the additional layers for inputs1\n",
    "        x1 = self.fc1(pooled_output1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        x1 = self.fc2(x1)\n",
    "        \n",
    "        # Pass through the additional layers for inputs2\n",
    "        x2 = self.fc1(pooled_output2)\n",
    "        x2 = self.relu(x2)\n",
    "        x2 = self.dropout(x2)\n",
    "        x2 = self.fc2(x2)\n",
    "        \n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "# TODO\n",
    "\n",
    "class CodePairDataset(Dataset):\n",
    "    def __init__(self, code_pairs, labels):\n",
    "        self.code_pairs = code_pairs\n",
    "        self.labels     = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        code1, code2 = self.code_pairs[idx]\n",
    "        label        = self.labels[idx]\n",
    "        return code1, code2, torch.tensor(label, dtype=torch.float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.code_pairs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv_data(cls, path: str = \"training.csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "        code_pairs, labels = [], []\n",
    "        # TODO: there must be a more efficient way to do this\n",
    "        for _, row in df.iterrows():\n",
    "            code_pairs.append((row['src_x'], row['src_y'])); labels.append(row['label'])\n",
    "        return cls(code_pairs, labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# TODO\n",
    "\n",
    "def train(model, dataloader, optimizer, epochs, margin):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for code1, code2, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get embeddings for both code snippets\n",
    "            embeddings1, embeddings2 = model(code1, code2)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = contrastive_loss(embeddings1, embeddings2, labels, margin)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, tokenizer, and optimizer\n",
    "model = CodeSimilarityModel(pretrained_model_name=\"microsoft/codebert-base\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "dataset = CodePairDataset.from_csv_data()\n",
    "dataloader = DataLoader(dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "epochs = 5; margin = 1.0\n",
    "train(model, dataloader, optimizer, epochs, margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation dataset\n",
    "\n",
    "# TODO: \n",
    "# If CodeNet is used for evaluation the code pairs can't be overlapping with training data !!!\n",
    "\n",
    "# placeholder\n",
    "test_code_pairs = [\n",
    "    (\"def add(a, b): return a + b\", \"def sum(x, y): return x + y\"), \n",
    "    (\"def add(a, b): return a + b\", \"def subtract(a, b): return a - b\"),\n",
    "#...\n",
    "]\n",
    "test_labels = [1, 0]  # Corresponding labels for the test pairs\n",
    "\n",
    "test_dataset = CodePairDataset(test_code_pairs, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# TODO\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_distances = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for code1, code2, labels in dataloader:\n",
    "            # Get embeddings for both code snippets\n",
    "            embeddings1, embeddings2 = model(code1, code2)\n",
    "            \n",
    "            # Compute the Euclidean distance between the embeddings\n",
    "            distances = F.pairwise_distance(embeddings1, embeddings2)\n",
    "            \n",
    "            # Store the distances and labels\n",
    "            all_distances.extend(distances.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return all_labels, all_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_labels, test_distances = evaluate(model, test_dataloader)\n",
    "\n",
    "# Print results\n",
    "for label, distance in zip(test_labels, test_distances):\n",
    "    print(f\"Label: {label}, Distance: {distance}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
